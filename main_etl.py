# main_etl.py

import os
import argparse
from datetime import datetime
import polars as pl

# å¼•å…¥æˆ‘å€‘å¯«å¥½çš„æ¨¡çµ„
from config.settings import DATA_ROOT, TIMEFRAMES
from adapters.shioaji_source import ShioajiSource
from core.resampler import resample_to_kbars

# å®šç¾©ç›®æ¨™å•†å“æ¸…å–®
TARGET_SYMBOLS = ['TXF', 'TSE']

def run_pipeline(date_str, shared_source=None):
    print(f"ğŸš€ Starting ETL Pipeline for {date_str}...")
    
    # ğŸŸ¢ [ä¿®æ”¹ 2] æ±ºå®šä½¿ç”¨å“ªå€‹ Source
    if shared_source is None:
        # å¦‚æœå¤–éƒ¨æ²’çµ¦ï¼Œå°±è‡ªå·±å»ºç«‹ä¸€å€‹ (å–®æ—¥æ¨¡å¼)
        source = ShioajiSource()
        is_local_session = True # æ¨™è¨˜é€™æ˜¯è‡ªå·±å»ºçš„ï¼Œç­‰ä¸‹è¦è² è²¬é—œæ‰
    else:
        # å¦‚æœå¤–éƒ¨æœ‰çµ¦ï¼Œå°±ç”¨å¤–éƒ¨çš„ (æ‰¹æ¬¡æ¨¡å¼)
        source = shared_source
        is_local_session = False # é€™æ˜¯åˆ¥äººå€Ÿæˆ‘çš„ï¼Œæˆ‘ä¸èƒ½é—œæ‰å®ƒ

    year = date_str[:4]
    month = date_str[5:7]

    try:
        # ç¢ºä¿é€£ç·š (ShioajiSource å…§éƒ¨æœ‰ checkï¼Œé‡è¤‡å‘¼å« connect æ²’æˆæœ¬)
        source.connect()

        for symbol in TARGET_SYMBOLS:
            print(f"\n------ Processing {symbol} ------")

            # 0. é å…ˆè¨ˆç®— Raw Data è·¯å¾‘
            raw_dir = os.path.join(DATA_ROOT, "raw_ticks", symbol, year, month)
            raw_path = os.path.join(raw_dir, f"{date_str}_{symbol}_ticks.parquet")
            
            tick_df = None

            # æª¢æŸ¥æœ¬åœ°æ˜¯å¦å·²æœ‰æª”æ¡ˆ
            if os.path.exists(raw_path):
                print(f"ğŸ“¦ Found local raw data: {raw_path}")
                print("   â© Skipping download, loading from disk...")
                try:
                    tick_df = pl.read_parquet(raw_path)
                except Exception as e:
                    print(f"âš ï¸ Local file corrupted ({e}), forcing re-download.")
            
            # å¦‚æœæœ¬åœ°æ²’æª”æ¡ˆ (tick_df é‚„æ˜¯ None)ï¼Œæ‰å»ç¶²è·¯ä¸‹è¼‰
            if tick_df is None:
                # --- Phase 1: Extract (ä¸‹è¼‰) ---
                tick_df = source.fetch_ticks(date_str, symbol)
                
                if tick_df.is_empty():
                    print(f"âš ï¸  No data found for {symbol} on {date_str}. Skipping.")
                    continue

                # --- Phase 2: Load Raw (å­˜æª”) ---
                os.makedirs(raw_dir, exist_ok=True)
                tick_df.write_parquet(raw_path)
                print(f"âœ… Raw Ticks downloaded & saved: {raw_path}")
            
            # --- Phase 3: Transform & Load K-Bars ---
            for tf in TIMEFRAMES:
                kbar_df = resample_to_kbars(tick_df, tf)
                
                if kbar_df.is_empty():
                    continue

                # [åˆ†æµå„²å­˜ç­–ç•¥] æ ¹æ“šé€±æœŸæ±ºå®šå„²å­˜ç­–ç•¥
                # Case A: æ—¥ç·š (1d) -> å­˜æˆã€Œå¹´æª”ã€ï¼Œä½¿ç”¨ Append æ¨¡å¼
                if tf == '1d':
                    kbar_dir = os.path.join(DATA_ROOT, "kbars", tf, symbol)
                    os.makedirs(kbar_dir, exist_ok=True)
                    
                    # æª”å: TXF_1d_2025.parquet
                    save_path = os.path.join(kbar_dir, f"{symbol}_{tf}_{year}.parquet")
                    
                    if os.path.exists(save_path):
                        # è®€å–èˆŠæª” -> åˆä½µ -> å»é‡ -> å¯«å›
                        try:
                            existing_df = pl.read_parquet(save_path)
                            # åˆä½µä¸¦ä»¥ ts å»é‡ (ä¿ç•™æœ€æ–°çš„)
                            final_df = pl.concat([existing_df, kbar_df]).unique(subset=["ts"], keep="last").sort("ts")
                        except Exception as e:
                            print(f"âš ï¸ Merge error, overwriting: {e}")
                            final_df = kbar_df
                    else:
                        final_df = kbar_df
                        
                    final_df.write_parquet(save_path)
                    print(f"   -> {tf} Updated: {save_path} (Total days: {len(final_df)//2})")

                # Case B: åˆ†æ™‚/åˆ†ç§’ (1m, 5s...) -> å­˜æˆã€Œæ—¥æª”ã€ï¼Œç›´æ¥è¦†è“‹
                else:
                    kbar_dir = os.path.join(DATA_ROOT, "kbars", tf, symbol, year)
                    os.makedirs(kbar_dir, exist_ok=True)
                    
                    save_path = os.path.join(kbar_dir, f"{date_str}_{symbol}_{tf}.parquet")
                    kbar_df.write_parquet(save_path)
                    print(f"   -> {tf} Saved: {save_path} ({len(kbar_df)} bars)")

    except Exception as e:
        print(f"âŒ ETL Failed: {e}")
    finally:
        # åªæœ‰çœŸæ­£é€£ç·šéæ‰éœ€è¦ç™»å‡º
        if is_local_session and source.is_connected:
            source.report_usage()
            source.logout()
            print("ğŸ‘‹ Shioaji Logout.")
        else:
            print("ğŸ”„ Keeping connection alive for next batch...")

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="TXF Data Lake ETL")
    default_date = datetime.now().strftime('%Y-%m-%d')
    parser.add_argument('--date', type=str, default=default_date, help='Format: YYYY-MM-DD')
    
    args = parser.parse_args()
    
    run_pipeline(args.date)